#!/usr/bin/env python3
"""
LAMARL çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
LLMç”Ÿæˆ + Prior Policyçµ±åˆ + Reward Functionçµ±åˆã®å‹•ä½œç¢ºèª
"""

import sys
import os
import json
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.env import SwarmEnv
from app.marl import MADDPGSystem
from app.metrics import coverage_m1, uniformity_m2
from app.llm.client import generate_prior_reward_dsl
from app.llm.dsl_runtime import build_prior_fn, build_reward_fn


def test_integration():
    """çµ±åˆãƒ†ã‚¹ãƒˆ: LLMç”Ÿæˆ + Prior Policy + Reward Function"""
    
    print("=" * 60)
    print("ğŸ§ª LAMARL çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # ç’°å¢ƒè¨­å®š
    print("\n[1] ç’°å¢ƒåˆæœŸåŒ–")
    env = SwarmEnv(
        shape="circle",
        grid_size=64,
        n_robot=10,  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°
        r_sense=0.4,
        r_avoid=0.1,
        nhn=6,
        nhc=80,
        l_cell=1.0,
        seed=42
    )
    obs = env.reset()
    print(f"âœ… ç’°å¢ƒåˆæœŸåŒ–å®Œäº†: {env.n}å°ã®ãƒ­ãƒœãƒƒãƒˆ, è¦³æ¸¬æ¬¡å…ƒ={obs.shape[1]}")
    
    # MADDPGåˆæœŸåŒ–
    print("\n[2] MADDPGåˆæœŸåŒ–")
    maddpg = MADDPGSystem(
        n_agents=env.n,
        obs_dim=obs.shape[1],
        gamma=0.99,
        batch=512,
        lr_actor=1e-4,
        lr_critic=1e-3,
        noise=0.1,
        tau=0.005,
        capacity=10_000,
        warmup_steps=100,  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ç¸®
        alpha_prior=0.1,  # Prioræ­£å‰‡åŒ–ä¿‚æ•°
        beta=0.3  # Priorèåˆä¿‚æ•°
    )
    print(f"âœ… MADDPGåˆæœŸåŒ–å®Œäº†: {len(maddpg.agents)}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    
    # LLMç”Ÿæˆ
    print("\n[3] LLMç”Ÿæˆ (Gemini API)")
    task_description = "10å°ã®ãƒ­ãƒœãƒƒãƒˆã§å††å½¢ã‚’å½¢æˆã—ã€å‡ç­‰ã«é…ç½®ã™ã‚‹"
    env_params = {
        "shape": "circle",
        "n_robot": 10,
        "r_sense": 0.4,
        "r_avoid": 0.1,
        "n_hn": 6,
        "n_hc": 80,
    }
    
    try:
        dsl = generate_prior_reward_dsl(
            task_description=task_description,
            env_params=env_params,
            model="gemini-2.0-flash-exp",
            temperature=0.7,
            use_cot=True,
            use_basic_apis=True
        )
        
        print(f"âœ… LLMç”Ÿæˆå®Œäº†")
        print(f"   Prior Policy: {len(dsl['prior']['terms'])}é …")
        print(f"   Reward Formula: {dsl['reward']['formula']}")
        
        # DSLã‹ã‚‰å®Ÿè¡Œå¯èƒ½ãªé–¢æ•°ã‚’æ§‹ç¯‰
        prior_fn = build_prior_fn(dsl["prior"])
        reward_fn = build_reward_fn(dsl["reward"])
        
        # MADDPGã«è¨­å®š
        maddpg.set_prior_policy(prior_fn)
        maddpg.set_reward_function(reward_fn)
        
        print(f"âœ… Prior/Rewardé–¢æ•°ã‚’MADDPGã«è¨­å®š")
        
    except Exception as e:
        print(f"âš ï¸ LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        print("   Mock LLMã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
        
        dsl = generate_prior_reward_dsl(
            task_description=task_description,
            env_params=env_params,
            model="mock",
            temperature=0.7,
            use_cot=True,
            use_basic_apis=True
        )
        
        prior_fn = build_prior_fn(dsl["prior"])
        reward_fn = build_reward_fn(dsl["reward"])
        maddpg.set_prior_policy(prior_fn)
        maddpg.set_reward_function(reward_fn)
        
        print(f"âœ… Mock Prior/Rewardé–¢æ•°ã‚’MADDPGã«è¨­å®š")
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆçŸ­ç¸®ç‰ˆï¼‰
    print("\n[4] å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆ10ã‚¹ãƒ†ãƒƒãƒ—ï¼‰")
    obs = env.reset()
    
    for step in range(10):
        # çŠ¶æ…‹è¾æ›¸ã‚’å–å¾—ï¼ˆPrior Policyç”¨ï¼‰
        state_dicts = env.get_state_dicts()
        
        # è¡Œå‹•é¸æŠï¼ˆPriorèåˆã‚ã‚Šï¼‰
        acts = maddpg.act(obs, deterministic=False, state_dicts=state_dicts)
        
        # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—
        nobs, col_pairs = env.step(acts)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        M1 = coverage_m1(env.mask, env.p, env.ra)
        M2 = uniformity_m2(env.p, env.mask)
        
        # Rewardè¨ˆç®—ï¼ˆLLMç”Ÿæˆé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
        n_collisions = len(col_pairs)
        rew_scalar = reward_fn({
            "coverage": float(M1),
            "uniformity": float(M2),
            "collisions": float(n_collisions)
        })
        
        # ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´
        for i in range(env.n):
            maddpg.buffers[i].push(
                obs[i], acts[i],
                np.array([rew_scalar], dtype=np.float32),
                nobs[i], np.array([0.0], dtype=np.float32)
            )
        
        # æ›´æ–°ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¾Œï¼‰
        upd = maddpg.step_update()
        
        print(f"   Step {step+1}: M1={M1:.3f}, M2={M2:.3f}, Reward={rew_scalar:.3f}, Collisions={n_collisions}")
        if upd:
            print(f"      Loss: Actor={upd['loss_actor']:.4f}, Critic={upd['loss_critic']:.4f}")
        
        obs = nobs
    
    print("\nâœ… å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Œäº†")
    
    # Prior Policyã®ãƒ†ã‚¹ãƒˆ
    print("\n[5] Prior Policyã®å‹•ä½œãƒ†ã‚¹ãƒˆ")
    state_dicts = env.get_state_dicts()
    
    for i in range(min(3, env.n)):
        prior_action = prior_fn(state_dicts[i])
        print(f"   Robot {i}: Prior Action = {prior_action}")
    
    print("\n" + "=" * 60)
    print("âœ… çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    print("=" * 60)
    print("\nğŸ“‹ ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   - ç’°å¢ƒåˆæœŸåŒ–: âœ…")
    print(f"   - MADDPGåˆæœŸåŒ–: âœ…")
    print(f"   - LLMç”Ÿæˆ: âœ…")
    print(f"   - Prior Policyçµ±åˆ: âœ…")
    print(f"   - Reward Functionçµ±åˆ: âœ…")
    print(f"   - å­¦ç¿’ãƒ«ãƒ¼ãƒ—: âœ…")
    print("\nğŸ‰ ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
    
    return True


if __name__ == "__main__":
    try:
        success = test_integration()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

