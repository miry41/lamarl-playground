# 🧩 LAMARL における MARL モジュールの詳細解説

---

## 1. 基本構造：MADDPG をベースとした協調強化学習

論文では、MARL モジュールは **multi-agent continuous control tasks**（連続行動空間の多ロボット協調制御）を対象とする。

> “We adopt the multi-agent deep deterministic policy gradient (MADDPG) framework as the baseline for cooperative control, where each agent maintains an actor–critic pair and learns through centralized training with decentralized execution.”  
> （各エージェントが Actor–Critic ペアを持ち、中央訓練＋分散実行の枠組みで学習を行う。）

### 構成要素
- 各ロボット = エージェント i  
- ネットワーク構造：  
  - Actor: \( \pi_{\theta_i}(s_i) \)  
  - Critic: \( Q_{\phi_i}(s_1,\dots,s_N, a_1,\dots,a_N) \)
- 学習形態：  
  - **訓練時**：全体状態と行動を共有（centralized）  
  - **実行時**：自分の観測のみで行動（decentralized）

---

## 2. Actor・Critic の更新式

> “The actor is optimized to maximize the Q-value estimated by the critic, while the critic minimizes the temporal-difference loss between predicted and target Q-values.”

### 標準的な MADDPG の式

- **Critic 更新**
  \[
  L_{critic} = \mathbb{E}[(Q_\phi(s,a) - y)^2], \quad
  y = r + \gamma Q_{\phi'}(s', a')
  \]

- **Actor 更新**
  \[
  L_{actor} = -\mathbb{E}[Q_\phi(s, \pi_\theta(s))]
  \]

---

## 3. LAMARL 特有の拡張（Prior Policy 結合）

> “We integrate the LLM-generated prior policy into the actor update, adding a cooperative regularization term that encourages the learned policy to stay close to the prior one.”

### 拡張後の Actor 損失関数
\[
L_{actor}^{LAMARL} =
- \mathbb{E}[Q_\phi(s, \pi_\theta(s))]
+ \alpha \| \pi_\theta(s) - \pi_{prior}(s) \|^2
\]

ここで：
- \( \pi_\theta \)：学習中の Actor ポリシー  
- \( \pi_{prior} \)：LLM 生成の事前ポリシー  
- \( \alpha \)：正則化係数（例：0.1）

👉 **Actor が LLM の提案した行動方針から過度に逸脱しないようにする。**

---

## 4. Replay Buffer と Target Networks

> “We maintain a shared replay buffer storing tuples (s, a, r, s′), updated by all agents, and use target networks for both actor and critic to stabilize learning.”

### 特徴
- 共通リプレイバッファ（全エージェント共有）  
- Target Network による安定化  
- LLM Prior からの生成サンプルも追加可能  
  > “Additionally, we insert samples generated by the LLM prior into the replay buffer to improve exploration efficiency.”

---

## 5. 行動選択時の合成

> “The final action is a weighted combination of the learned actor output and the LLM prior output.”

\[
a_i = (1 - \beta) \pi_\theta(s_i) + \beta \pi_{prior}(s_i)
\]

- \(\beta\)：融合係数（0.0〜1.0）

学習段階では Actor が主導、実行段階では Prior が補助的にバイアスを与える。

---

## 6. Critic の形式（集中 or 局所）

> “For scalability, we replace the global critic with a local critic that only takes neighboring agents’ states and actions.”

### 形式
- **Global Critic:** 全体状態・行動を入力  
- **Local Critic:** 近傍エージェントのみを考慮（スケール改善）

---

## 7. ハイパーパラメータ（論文表より）

| パラメータ | 値 |
|-------------|----|
| Episodes | 3000 |
| Episode length | 200 |
| Batch size | 512 |
| γ (discount) | 0.99 |
| Learning rate (actor/critic) | 1e-4 / 1e-3 |
| Hidden layers | 3 |
| Hidden units | 180 |
| Exploration rate | 0.6 |
| Noise scale | 0.1 |
| α (prior重み) | 0.1 |

---

## 8. 成果・効果

> “LAMARL improves sample efficiency by ≈185.9% and the LLM output success rates by 28.5–67.5% compared with vanilla MADDPG.”

### 改善点
- サンプル効率：**約1.86倍向上**  
- 成功率：**28〜68% 向上**  
- 報酬設計の自動化と協調動作の安定化を実現

---

## 9. 数式まとめ

```text
# critic 更新
Lcritic = (Qφ(s,a) - [r + γ Qφ'(s', a')])^2

# actor 更新 (LAMARL 拡張)
Lactor = -Qφ(s, πθ(s)) + α * || πθ(s) - πprior(s) ||^2

# 実行時の行動決定
a = (1 - β) * πθ(s) + β * πprior(s)
```
## 10. 処理フロー要約

| 段階 | 処理内容 | 論文での特徴 |
|------|------------|--------------|
| Actor 更新 | MADDPGベース + Prior正則化 | LLM出力を「行動バイアス」に導入 |
| Critic 更新 | TD誤差最小化 | Global/Local Criticの選択 |
| Replay Buffer | 経験＋Priorサンプル | 探索効率向上 |
| 行動決定 | ActorとPriorの線形合成 | Cooperative Policy生成 |
| 安定化技術 | Targetネット＋ノイズ制御 | 学習の安定性確保 |

## 11. 実装の全体イメージ（疑似コード）
```
for episode in range(E):
    for t in range(T):
        # 1. 各ロボットの観測から行動決定
        a_prior = prior_policy(state)
        a_actor = actor_network(state)
        action = (1 - beta) * a_actor + beta * a_prior

        # 2. 環境を進める
        next_state, reward, done = env.step(action)

        # 3. リプレイバッファに格納
        buffer.append((state, action, reward, next_state))

        # 4. 批評家の更新
        critic_loss = mse(Q(s,a), r + gamma * Q'(s', a'))
        update(critic, critic_loss)

        # 5. 俳優の更新（Prior正則化付き）
        actor_loss = -Q(s, πθ(s)) + α * ||πθ(s) - πprior(s)||^2
        update(actor, actor_loss)

        state = next_state
```
## 12. まとめ：MARL モジュールの役割

- **LLMモジュールで生成された prior_policy と reward_function を受け取り、学習ループに統合する。**
- **Actor** は “LLMの方針をなぞりつつ最適化”  
- **Critic** は “協調行動を定量的に評価”  
- **Replay Buffer** は “実経験とLLM生成経験を両方学習に活かす”

---

> **結論:**  
> LAMARL の MARL モジュールは、MADDPG を基盤にしつつ、  
> LLM が生成した「事前知識（ポリシー・報酬）」を組み込むことで、  
> 協調的な多エージェント行動を効率的に学習する仕組みである。
