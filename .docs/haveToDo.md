# backend å®Ÿè£…ãƒã‚§ãƒƒã‚¯çµæœ

## ğŸ”´ **æœ€å„ªå…ˆã§ä¿®æ­£ãŒå¿…è¦ãªå•é¡Œ**

### 1. Prior Policy çµ±åˆãŒæœªå®Ÿè£…ï¼ˆLAMARL ã®æ ¸å¿ƒæ©Ÿèƒ½ï¼‰

è«–æ–‡ã®æœ€ã‚‚é‡è¦ãªç‰¹å¾´ã§ã‚ã‚‹ Prior Policy ã®çµ±åˆãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

#### å•é¡Œç®‡æ‰€ã¨å¿…è¦ãªä¿®æ­£:

**â‘  Actor æå¤±ã®æ­£å‰‡åŒ–é …ãŒæ¬ è½** (`marl.py:91`)
```python
# ç¾åœ¨ã®å®Ÿè£…
loss_a = - self.critic(torch.cat([obs, a], dim=1)).mean()

# å¿…è¦ãªå®Ÿè£…ï¼ˆè«–æ–‡ d3_marlModule.md å‚ç…§ï¼‰
# Lactor = -QÏ†(s, Ï€Î¸(s)) + Î± * || Ï€Î¸(s) - Ï€prior(s) ||^2
```

**â‘¡ è¡Œå‹•æ±ºå®šæ™‚ã® prior åˆæˆãŒãªã„** (`marl.py:52-62`)
```python
# è«–æ–‡ d3_marlModule.md L78-80
# a = (1 - Î²) * Ï€Î¸(s) + Î² * Ï€prior(s)
```

**å¿…è¦ãªä½œæ¥­:**
- `MADDPGAgent.update()` ã« `prior_action` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ `alpha_prior` é‡ã¿ã‚’è¿½åŠ 
- `MADDPGAgent.act()` ã« `prior_policy` é–¢æ•°ã¨ `beta` èåˆä¿‚æ•°ã‚’è¿½åŠ 
- `MADDPGSystem` ã« prior_policy é–¢æ•°ã‚’ä¿æŒãƒ»é©ç”¨ã™ã‚‹ä»•çµ„ã¿ã‚’è¿½åŠ 

---

### 2. å ±é…¬é–¢æ•°ãŒå›ºå®šï¼ˆLLM ç”Ÿæˆã‚’çµ±åˆã§ããªã„ï¼‰

**ç¾åœ¨ã®å®Ÿè£…** (`main.py:179-180`)
```python
done = 1.0 if (M1 > 0.8 and M2 < 0.2) else 0.0
rew_scalar = 1.0 if done == 1.0 else 0.0
```

**å•é¡Œç‚¹:**
- ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã®ã¿ï¼ˆæˆåŠŸ/å¤±æ•—ã®2å€¤ï¼‰
- LLMç”Ÿæˆã® `reward_function(global_state)` ã‚’å·®ã—æ›¿ãˆã‚‹ãƒ•ãƒƒã‚¯ãŒãªã„

**æ¨å¥¨å¯¾å¿œ:**
- `store["reward_fn"]` ã¨ã—ã¦å ±é…¬é–¢æ•°ã‚’ä¿æŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨ã®ã‚¹ãƒ‘ãƒ¼ã‚¹å ±é…¬ã€LLMç”Ÿæˆå¾Œã¯å·®ã—æ›¿ãˆå¯èƒ½ã«

---

### 3. çµæœä¿å­˜ãŒæœªå®Ÿè£…

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦æ±‚** (`d4_evaluation.md`)
- `metrics.json` ã®ä¿å­˜ï¼ˆM1/M2 ã®æ¨ç§»ãƒ‡ãƒ¼ã‚¿ï¼‰
- `final_shape.png` ã®ä¿å­˜ï¼ˆæœ€çµ‚é…ç½®ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼‰

**ç¾çŠ¶:**
- SSE ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’æµã™ã ã‘ã§æ°¸ç¶šåŒ–ã•ã‚Œãªã„
- `_train_loop()` ã® L216-222 ãŒæœªå®Œæˆ

**å¿…è¦ãªä½œæ¥­:**
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã« JSON ã¨PNG ã‚’ä¿å­˜
- PIL ã§å¯è¦–åŒ–ç”»åƒã‚’ç”Ÿæˆï¼ˆãƒ­ãƒœãƒƒãƒˆä½ç½® + å½¢çŠ¶ãƒã‚¹ã‚¯ï¼‰

---

## ğŸŸ  **è»½å¾®ãªå•é¡Œï¼ˆå‹•ä½œã«ã¯å½±éŸ¿ã—ãªã„ï¼‰**

### 4. è¦³æ¸¬ã®ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ãŒæ›–æ˜§

**è©²å½“ç®‡æ‰€** (`env.py:63`)
```python
if dist[j] <= self.rs * self.grid_size/8 and cnt < self.nhn:
```

**å•é¡Œç‚¹:**
- `grid_size/8` ã¨ã„ã†çµŒé¨“çš„ä¿‚æ•°ã®æ„å›³ãŒä¸æ˜
- ç‰©ç†å˜ä½ï¼ˆmï¼‰ã¨ã‚°ãƒªãƒƒãƒ‰åº§æ¨™ã®å¯¾å¿œãŒæ–‡æ›¸åŒ–ã•ã‚Œã¦ã„ãªã„

**æ¨å¥¨å¯¾å¿œ:**
- ã‚³ãƒ¡ãƒ³ãƒˆã§ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã®ç†ç”±ã‚’æ˜è¨˜
- ã¾ãŸã¯ `r_sense_grid = self.rs * (self.grid_size / physical_size)` ã®ã‚ˆã†ã«æ˜ç¤ºçš„ã«è¨ˆç®—

---

### 5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰

**è©²å½“ç®‡æ‰€** (`marl.py:4`)
```python
def mlp(in_dim, out_dim, hidden=[180,180,180], out_act=None):
```

**ç¾çŠ¶:**
- hidden_dim=180, hidden_layers=3 ãŒå›ºå®š
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆd1_env-input.mdï¼‰ã§ã¯å¯å¤‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜è¼‰

**å¯¾å¿œ:**
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«ã‚ˆã‚Šã€Œãƒã‚¤ãƒ‘ãƒ©ã¯å›ºå®šã§ã„ã„ã€ã¨ã®ã“ã¨ãªã®ã§ç¾çŠ¶ç¶­æŒã§OK
- å°†æ¥çš„ã«å¯å¤‰ã«ã™ã‚‹å ´åˆã¯ `MADDPGAgent.__init__()` ã§å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´

---

## âœ… **æ­£å¸¸ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†**

- ç’°å¢ƒ (env.py): shape, n_robot, è¦³æ¸¬ã®å›ºå®šæ¬¡å…ƒåŒ–ãªã©
- MARLåŸºç¤ (marl.py): MADDPGæ§‹é€ ã€Actor/Criticã€Target Network
- è©•ä¾¡æŒ‡æ¨™ (metrics.py): Coverage (M1) ã¨ Uniformity (M2)
- å½¢çŠ¶ç”Ÿæˆ (shapes.py): circle, L, A, M, R ãªã©
- Replay Buffer (buffer.py): ã‚·ãƒ³ãƒ—ãƒ«ã§æ­£å¸¸
- APIæ§‹é€  (main.py): FastAPI + SSE ã®æ çµ„ã¿

---

## ğŸ“‹ ä¿®æ­£å„ªå…ˆåº¦ã¾ã¨ã‚

| å„ªå…ˆåº¦ | é …ç›® | å½±éŸ¿ |
|--------|------|------|
| ğŸ”´ æœ€é«˜ | Prior Policy çµ±åˆ | LAMARL ã®æœ¬è³ªçš„æ©Ÿèƒ½ãŒæ¬ è½ |
| ğŸŸ¡ é«˜ | å ±é…¬é–¢æ•°ã®å¯å¤‰åŒ– | LLMçµ±åˆæ™‚ã«å¿…è¦ |
| ğŸŸ¡ é«˜ | çµæœä¿å­˜ã®å®Ÿè£… | è©•ä¾¡ãƒ»ãƒ‡ãƒãƒƒã‚°ã«å¿…è¦ |
| ğŸŸ  ä½ | ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ã®æ–‡æ›¸åŒ– | å¯èª­æ€§å‘ä¸Š |
| ğŸŸ  ä½ | ãƒã‚¤ãƒ‘ãƒ©ã®å¯å¤‰åŒ– | ç¾çŠ¶ã¯ä¸è¦ï¼ˆå›ºå®šã§OKï¼‰ |

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å³åº§ã«ä¿®æ­£**: main.py L216 ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£
2. **LAMARLå®Ÿè£…**: Prior Policy çµ±åˆï¼ˆÎ±æ­£å‰‡åŒ– + Î²åˆæˆï¼‰
3. **çµæœä¿å­˜**: metrics.json / final_shape.png ã®å‡ºåŠ›
4. **LLMæº–å‚™**: reward_function ã®å·®ã—æ›¿ãˆå¯èƒ½ãªè¨­è¨ˆ

ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å˜ä½“ã§ã®å‹•ä½œç¢ºèªã«ã¯ **1ã¨2** ãŒå¿…é ˆã§ã™ã€‚
