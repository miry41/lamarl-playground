# ğŸ§© LAMARL ã® Prompt & Basic API ã«é–¢ã™ã‚‹è¨˜è¿°
## Prompt Designï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆï¼‰
### æ¦‚è¦
è«–æ–‡ã®è©²å½“ç®‡æ‰€ã§ã¯ã€ŒLLM-assisted moduleã€å†…ã§èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚  
LLMã«ã¯ã€Œè‡ªç„¶è¨€èªã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯è¨˜è¿°ï¼‹APIä»•æ§˜ã€ã‚’å…¥åŠ›ã—ã€ãã“ã‹ã‚‰ Python ã‚³ãƒ¼ãƒ‰ï¼ˆpolicy/rewardï¼‰ã‚’ç”Ÿæˆã•ã›ã‚‹ã¨ã„ã†è¨­è¨ˆã§ã™ã€‚

---

### è«–æ–‡è¨˜è¿°ï¼ˆè¦ç´„ï¼‰

> The LLM-aided module receives a task description  
> (e.g., â€œForm a circle shape using multiple robotsâ€)  
> and basic API documentation as context.  
> It generates both the prior policy Ï€_prior(s) and reward function R(s,a)  
> through a chain-of-thought reasoning process.

> The prompt template includes:  
> - Task description in natural language  
> - State and action variable definitions  
> - API reference for perception and actuation  
> - Output format requirements (Python functions)

---

# ğŸ§  LAMARL ã«ãŠã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆï¼ˆå…¨3ç¨®ï¼‰

---

## â‘  Policy Generation Promptï¼ˆäº‹å‰ãƒãƒªã‚·ãƒ¼ç”Ÿæˆï¼‰

**ç›®çš„**ï¼šãƒ­ãƒœãƒƒãƒˆã®è¡Œå‹•æ–¹é‡ Ï€_prior(s) ã‚’ç”Ÿæˆã™ã‚‹  
**å‡ºåŠ›**ï¼š`def prior_policy(state): ...`

```
You are an expert in multi-robot reinforcement learning.
Task: Form a "circle" shape with 30 robots.
Robots can sense neighbors within 0.4 m and avoid collisions within 0.1 m.
Use the provided API functions to compute attraction, repulsion, and synchronization forces.

Available APIs:
- get_neighbors(state)
- get_distance(a, b)
- get_direction(a, b)
- avoid_collision(state)
- synchronize_velocity(state)
- attract_to_goal(state)

Return Python code implementing:
def prior_policy(state): ...

```
---

## â‘¡ Reward Design Promptï¼ˆå ±é…¬é–¢æ•°ç”Ÿæˆï¼‰

**ç›®çš„**ï¼šå ±é…¬é–¢æ•° R(s,a) ã‚’è¨­è¨ˆã™ã‚‹  
**å‡ºåŠ›**ï¼šdef reward_function(global_state): ...
```
Design a reward function R(s,a) that encourages robots to form the target shape.
Use the provided APIs for measuring performance.

Available APIs:
- compute_coverage(global_state)
- compute_uniformity(global_state)

Return Python code implementing:
def reward_function(global_state): ...
```
## â‘¢ Review / Chain-of-Thought Promptï¼ˆè‡ªå·±è©•ä¾¡ãƒ»æ”¹å–„ææ¡ˆï¼‰

**ç›®çš„**ï¼šç”Ÿæˆã—ãŸãƒãƒªã‚·ãƒ¼ã¨å ±é…¬ã‚’è‡ªå·±ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ç†ç”±ã¨æ”¹å–„ç‚¹ã‚’å‡ºåŠ›ã™ã‚‹  
**å‡ºåŠ›**ï¼šè‡ªç„¶è¨€èªã«ã‚ˆã‚‹èª¬æ˜æ–‡ï¼ˆCoTï¼‰
```
Review the generated prior_policy and reward_function.
Explain why each term is used and how it contributes to cooperative behavior.
If potential issues exist, suggest improvements.
Return your reasoning as a step-by-step explanation.
```
## âœ… ã¾ã¨ã‚
| No | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå | ç›®çš„ | å‡ºåŠ›å½¢å¼ |
|----|---------------|------|-----------|
| â‘  | Policy Generation | äº‹å‰ãƒãƒªã‚·ãƒ¼ç”Ÿæˆ | Pythoné–¢æ•° |
| â‘¡ | Reward Design | å ±é…¬é–¢æ•°ç”Ÿæˆ | Pythoné–¢æ•° |
| â‘¢ | Review / CoT | è‡ªå·±è©•ä¾¡ã¨æ”¹å–„ææ¡ˆ | è‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆ |
