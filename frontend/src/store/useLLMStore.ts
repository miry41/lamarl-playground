/**
 * LLM Generation State Store (Zustand)
 * LLMã«ã‚ˆã‚‹ Prior Policy / Reward Function ç”Ÿæˆã®çŠ¶æ…‹ç®¡ç†
 */

import { create } from 'zustand'

// ==================== Types ====================

export interface PriorTerm {
  op: string
  weight: number
  radius?: number
  cell_size?: number
}

export interface PriorDSL {
  type: 'prior_policy_v1'
  combination: 'weighted_sum'
  terms: PriorTerm[]
  clamp: { max_speed: number }
}

export interface RewardDSL {
  type: 'reward_v1'
  formula: string
  clamp: { min: number; max: number }
}

export interface GenerationResult {
  prior: PriorDSL
  reward: RewardDSL
  cot_reasoning?: string
  metadata?: {
    model?: string
    temperature?: number
    usage?: {
      prompt_tokens?: number
      completion_tokens?: number
      total_tokens?: number
    }
  }
}

export interface GenerationRequest {
  task_description: string
  shape: string
  n_robot: number
  r_sense: number
  r_avoid: number
  n_hn: number
  n_hc: number
  use_cot: boolean
  use_basic_apis: boolean
  model: string
  temperature: number
}

export interface LLMState {
  // ç”ŸæˆçŠ¶æ…‹
  isGenerating: boolean
  generationProgress: 'idle' | 'input' | 'analysis' | 'generation' | 'review' | 'completed'
  
  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  request: GenerationRequest
  
  // ç”Ÿæˆçµæœ
  result: GenerationResult | null
  
  // ç”Ÿæˆå±¥æ­´ï¼ˆæœ€æ–°5ä»¶ï¼‰
  history: GenerationResult[]
  
  // ã‚¨ãƒ©ãƒ¼
  error: string | null
}

interface LLMStore extends LLMState {
  // Actions
  setRequest: (req: Partial<GenerationRequest>) => void
  generate: () => Promise<void>
  clearResult: () => void
  clearError: () => void
  
  // Internal
  _setProgress: (progress: LLMState['generationProgress']) => void
}

// ==================== Store ====================

const defaultRequest: GenerationRequest = {
  task_description: 'Assemble robots into the selected shape with uniform distribution and no collisions',
  shape: 'circle',  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯circle
  n_robot: 30,
  r_sense: 0.4,  // max: 1.0
  r_avoid: 0.1,  // max: 0.5
  n_hn: 6,       // max: 20
  n_hc: 80,      // max: 200
  use_cot: true,
  use_basic_apis: true,
  model: 'gemini-2.5-flash',  // Gemini-2.5-flashã«å›ºå®š
  temperature: 0.7,
}

// å€¤ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚¯ãƒ©ãƒ³ãƒ—ï¼‰ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
const clampRequest = (req: GenerationRequest): GenerationRequest => {
  const clamped = {
    ...req,
    r_sense: Math.max(0.1, Math.min(1.0, req.r_sense)),
    r_avoid: Math.max(0.01, Math.min(0.5, req.r_avoid)),
    n_robot: Math.max(1, Math.min(100, req.n_robot)),
    n_hn: Math.max(1, Math.min(20, req.n_hn)),
    n_hc: Math.max(10, Math.min(200, req.n_hc)),
    temperature: Math.max(0, Math.min(2, req.temperature)),
  }
  
  // å€¤ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿ï¼‰
  // if (req.r_sense !== clamped.r_sense || req.r_avoid !== clamped.r_avoid ||
  //     req.n_robot !== clamped.n_robot || req.n_hn !== clamped.n_hn ||
  //     req.n_hc !== clamped.n_hc || req.temperature !== clamped.temperature) {
  //   console.warn('ğŸ”§ LLM request values clamped:', {
  //     r_sense: `${req.r_sense} â†’ ${clamped.r_sense}`,
  //     r_avoid: `${req.r_avoid} â†’ ${clamped.r_avoid}`,
  //     n_robot: `${req.n_robot} â†’ ${clamped.n_robot}`,
  //     n_hn: `${req.n_hn} â†’ ${clamped.n_hn}`,
  //     n_hc: `${req.n_hc} â†’ ${clamped.n_hc}`,
  //     temperature: `${req.temperature} â†’ ${clamped.temperature}`,
  //   })
  // }
  
  return clamped
}

// console.log('ğŸª Initializing LLM Store with:', clampRequest(defaultRequest))

export const useLLMStore = create<LLMStore>((set, get) => ({
  // Initial state
  isGenerating: false,
  generationProgress: 'idle',
  request: clampRequest(defaultRequest),  // åˆæœŸå€¤ã‚‚ã‚¯ãƒ©ãƒ³ãƒ—
  result: null,
  history: [],
  error: null,

  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
  setRequest: (req) => {
    const updatedReq = clampRequest({ ...get().request, ...req })
    set({ request: updatedReq })
    // Note: MARLã‚¿ãƒ–ã®ç’°å¢ƒè¨­å®šã¯ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®SSE (env_config) ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™
  },

  // LLMç”Ÿæˆã‚’é–‹å§‹
  generate: async () => {
    const { request } = get()
    
    try {
      set({ 
        isGenerating: true, 
        generationProgress: 'input',
        error: null 
      })

      // ã‚¹ãƒ†ãƒƒãƒ—1: å…¥åŠ›å—ä»˜
      await new Promise(resolve => setTimeout(resolve, 500))
      set({ generationProgress: 'analysis' })

      // ã‚¹ãƒ†ãƒƒãƒ—2: åˆ¶ç´„åˆ†æ
      await new Promise(resolve => setTimeout(resolve, 1000))
      set({ generationProgress: 'generation' })

      // ã‚¹ãƒ†ãƒƒãƒ—3: é–¢æ•°ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å‘¼ã³å‡ºã—ï¼‰
      // console.log('ğŸš€ Sending LLM generation request:', request)
      const API_BASE = import.meta.env.VITE_API_URL || 'http://localhost:8000'
      const response = await fetch(`${API_BASE}/llm/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(request),
      })

      if (!response.ok) {
        const error = await response.json()
        console.error('âŒ LLM API Error:', error)
        const errorMsg = typeof error.detail === 'string' 
          ? error.detail 
          : JSON.stringify(error.detail || error)
        throw new Error(errorMsg)
      }

      const result: GenerationResult = await response.json()

      // ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¬ãƒ“ãƒ¥ãƒ¼
      set({ generationProgress: 'review' })
      await new Promise(resolve => setTimeout(resolve, 1000))

      // å®Œäº†
      set({ 
        result,
        history: [result, ...get().history].slice(0, 5), // æœ€æ–°5ä»¶ã‚’ä¿æŒ
        generationProgress: 'completed',
        isGenerating: false 
      })

      // console.log('âœ… LLM generation completed:', result)

    } catch (err) {
      const message = err instanceof Error ? err.message : 'Unknown error'
      set({ 
        error: message,
        isGenerating: false,
        generationProgress: 'idle'
      })
      console.error('âŒ LLM generation failed:', err)
    }
  },

  // çµæœã‚’ã‚¯ãƒªã‚¢
  clearResult: () => {
    set({ 
      result: null, 
      generationProgress: 'idle' 
    })
  },

  // ã‚¨ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
  clearError: () => {
    set({ error: null })
  },

  // å†…éƒ¨: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
  _setProgress: (progress) => {
    set({ generationProgress: progress })
  },
}))

